Introduction
------------
Credit card fraud is a major financial threat affecting millions of transactions worldwide. 
The goal of this project is to build a machine learning model that detects fraudulent 
transactions with high reliability. Due to the rarity of fraud in real-world data, the 
challenge is not only to classify correctly but also to handle extreme class imbalance.

Project Goal
------------
To develop a fraud detection model that:
- Identifies fraudulent transactions accurately
- Minimizes false negatives (missed fraud cases)
- Handles imbalanced data effectively
- Uses probability threshold tuning to improve recall

Dataset Details
---------------
The dataset used is the "Credit Card Fraud Detection" dataset from Kaggle.
It contains 284,807 transactions made by European cardholders in 2013.

Columns:
- V1 to V28: PCA-transformed confidential features
- Time: Seconds elapsed between transactions
- Amount: Transaction value
- Class: Target variable (0 = Non-Fraud, 1 = Fraud)

Why Fraud Detection is Hard
---------------------------
Fraud cases are extremely rare (about 0.17 percent of all transactions).
This rarity makes:
- Models biased toward predicting "Not Fraud"
- Accuracy misleading (99.8 percent accuracy is possible by predicting all 0s)
- Detecting the minority class difficult without special handling

Imbalanced Dataset Challenge
----------------------------
An imbalanced dataset means:
- Very few fraud cases
- Many normal transactions

Standard models fail because:
- They learn only the majority class
- They ignore the minority class
- They give high accuracy but bad fraud detection

Therefore, techniques like:
- class_weight balancing
- scaling
- threshold tuning
are required.

Import Libraries
----------------
We will import essential Python libraries for:
- Data cleaning (pandas, numpy)
- Scaling (StandardScaler)
- Modeling (LogisticRegression)
- Metrics (recall, confusion matrix)
- Visualization (matplotlib)

Load Dataset
------------
The CSV file "creditcard.csv" is loaded into a pandas DataFrame.
This begins the analysis pipeline.

Exploratory Data Analysis
-------------------------
The purpose of EDA is to understand data behavior.

Fraud Counts:
- Count how many fraud vs non-fraud samples exist.

Fraud vs Non-Fraud Distribution:
- Visualize imbalance
- Understand dataset difficulty

Check Duplicates:
- Identify if any repeated transactions exist

Check Missing Values:
- Ensure dataset cleanliness
- Missing data can break model training

Data Cleaning
-------------
Remove Duplicates:
- Drop any repeated rows to avoid bias

Confirm No Missing Values:
- Double-check that data is complete
- Fraud datasets usually have no missing values

Feature/Target Split
--------------------
Separate:
- X = all features except Class
- y = target column (Class)

This separation is essential for model training.

Scaling (StandardScaler)
------------------------
Scaling standardizes features to mean 0 and variance 1.

Why Scaling is Required:
- Logistic Regression, SVM, PCA, KMeans all require scaled data
- Prevents features with large numerical ranges from dominating
- Improves model convergence and stability

Train-Test Split
----------------
Split data into:
- Training: 80 percent
- Testing: 20 percent

Stratification Explanation:
- stratify=y keeps the same fraud ratio in both train and test sets
- Prevents the model from seeing only normal transactions during training

Model Training (Logistic Regression + class_weight)
---------------------------------------------------
Logistic Regression is used because:
- Works well on linear separable data
- Fast, stable, interpretable
- Good baseline model for fraud detection
- Supports class_weight balancing

Why class_weight="balanced":
- Forces the model to pay MORE attention to fraud samples
- Prevents fraud being ignored due to imbalance

Evaluation at Default Threshold (0.5)
-------------------------------------
Metrics evaluated:
- Confusion Matrix
- Classification Report (precision, recall, f1-score)
- ROC-AUC Score

Default 0.5 threshold usually:
- Misses many fraud cases
- Gives low recall
- Needs improvement

Threshold Tuning
----------------
Instead of using the default threshold, we test:
Thresholds from 0.001 to 0.20

Goal:
- Maximize recall
- Reduce false negatives
- Ensure no frauds are missed

Track Best Recall:
- For each threshold, calculate recall
- Select threshold with highest recall

Final Best Threshold
--------------------
Threshold Chosen:
- The threshold that achieved maximum recall (often around 0.001)

Final Confusion Matrix:
- Shows how many frauds detected
- Shows false positives count

Final Recall:
- Measure of how many fraud cases were caught
- Recall = 1.0 means no fraud case was missed

Conclusion
----------
Final Model Performance Summary:
- Achieved very high recall
- Successfully addressed class imbalance
- Logistic Regression with balanced weights performed strongly
- Threshold tuning improved fraud detection

Limitations:
- Higher false positives at low thresholds
- Needs better precision for real banking deployment
- PCA-transformed data limits feature interpretability

Possible Improvements:
----------------------
- Try tree-based models (RandomForest, XGBoost)
- Apply SMOTE for oversampling minority class
- Use cost-sensitive learning
- Experiment with neural networks
- Build a REST API for deployment
